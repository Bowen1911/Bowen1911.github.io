@inproceedings{lee-etal-2025-quantification,
    title = "Quantification of Large Language Model Distillation",
    author = "Lee, Sunbowen  and
      Zhou, Junting  and
      Ao, Chang  and
      Li, Kaige  and
      Du, Xeron  and
      He, Sirui  and
      Wu, Haihong  and
      Liu, Tianci  and
      Liu, Jiaheng  and
      Alinejad-Rokny, Hamid  and
      Yang, Min  and
      Liang, Yitao  and
      Wen, Zhoufutu  and
      Ni, Shiwen",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.acl-long.248/",
    doi = "10.18653/v1/2025.acl-long.248",
    pages = "4985--5004",
    ISBN = "979-8-89176-251-0",
    abstract = "Model distillation is a fundamental technique in building large language models (LLMs), transferring knowledge from a teacher model to a student model. However, distillation can lead to model homogenization, reducing diversity among models and impairing their ability to robustly handle complex or novel tasks. These limitations underscore the need to systematically quantify the distillation process and its impact. In this work, we propose a framework to evaluate and quantify model distillation. Our method addresses two key aspects: (1) Identifying identity cognition contradictions to assess discrepancies in how models perceive and represent identity-related information, and (2) Analyzing multi-granularity response similarities across models to measure the extent of homogenization. Experimental results demonstrate two key insights: (1) Well-known closed-source and open-source LLMs usually exhibit high distillation degrees, except for Claude, Doubao, and Gemini. (2) Base LLMs show higher distillation degrees compared to aligned LLMs. By offering a systematic approach to improve the transparency of LLM data distillation, we call for LLMs with more independent development and more transparent technical reports to improve LLMs' robustness and safety. The code and data are available at https://github.com/Aegis1863/LLMs-Distillation-Quantification."
}